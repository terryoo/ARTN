# ARTN
Deep Temporal Networks for Video Compression Artifact Reduction

<br><br>

## Abstract

It has been shown that deep convolutional neuralnetworks (CNN) reduce JPEG compression artifacts better thanthe previous approaches. However, the latest video compression standards have more complex artifacts including the flickering which are not well reduced by the CNN-based methods developed for still images. Also, recent video compression algorithms include in-loop filters which reduce the blocking artifacts, and thus post-processing barely improves the performance. In this paper, we propose a temporal-CNN architecture to reduce the artifacts in video compression standards as well as in JPEG. Specifically, we exploit a simple CNN structure and introduce a new training strategy that captures the temporal correlation of the consecutive frames in videos. The similar patches are aggregated from the neighboring frames by a simple motion search method, and they are fed to the CNN, which further reduces the artifacts within a frame and suppresses the flickering artifacts. Experiments show that our approach shows improvements over the conventional CNN-based methods with similar complexities, for image and video compression standards such as JPEG, MPEG-2, H.264/AVC, and HEVC.

<br><br>
## Related Work
### JPEG Artifact Reduction
#### [AR-CNN] Deep Convolution Networks for Compression Artifacts Reduction <paper-button> <a href="http://mmlab.ie.cuhk.edu.hk/projects/ARCNN.html">Link</a> </paper-button>
### HEVC Intra
#### [VRCNN] A Convolutional Neural Network Approach for Post-Processing in HEVC Intra Coding <paper-button> <a href="https://arxiv.org/pdf/1608.06690.pdf"> Link</a> </paper-button>
### CNN-based HEVC In-loop filter
#### [IFCNN] CNN-based in-loop filtering for coding efficiency improvement <paper-button> <a href="http://ieeexplore.ieee.org/document/7528223/"> Link</a> </paper-button>


<br><br>
## Deep Temporal Network
<embed src="file_name.pdf" width="800px" height="2100px" />
<p align="center"><img src="/figure/overall.pdf" width="700"></p>
The overall framework of the proposed network, where k, n, s above the convolution layers denote the kernel size, thenumber of output feature maps, and the convolution strides, respectively.
<br><br>

<p align="center">
<img src="/Figures/Fig_HDRI_Comparison.png" width="700"> 
</p>
Visual comparison of enhanced images from the MMPSG LDR images. (From left to right) input LDR images, enhanced images by FbEM, LIME, NPEA and the proposed algorithm.
<br>

#### □ Super-resolved results by VDSR
<p align="center">
<img src="/Figures/Proposed_VDSR.png" width="700"> 
</p>
HR images by interpolation and VDSR; (from left to right) interpolated HR image, input HR image and super-resolved HR image by VDSR.
<br>

#### □ HDR-SR test results
Visual comparison for an image of natural scene, (a) reference LDR/HR image, (b) interpolated LDR/HR image, (c) super-resolved LDR/HR image, (d-e) HDR/HR images generated by SR(YpUV)-HDR and SR(RGB)-HDR strategies, (f-g) HDR/HR images generated by HDR-SR(YpUV) and HDR-SR(RGB) strategies, (h-n) magnified images of the indicated areas in each image of (a-g).

<p align="center">
<img src="/s/small_Exp_VisualComp_C11.png" width="500"> 
</p>
*this is small-size sample image, please download the original image to compare the results.
<br>
